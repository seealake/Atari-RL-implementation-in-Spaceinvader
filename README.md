# Atari RL Implementation - Space Invaders

A Deep Reinforcement Learning implementation for playing Atari Space Invaders using various DQN (Deep Q-Network) variants.

## ğŸ® Overview

This project implements several variants of the Deep Q-Network algorithm to train an agent to play the classic Atari game Space Invaders. The implementation is based on the seminal paper "Human-Level Control Through Deep Reinforcement Learning" by Mnih et al. (2015).

### Supported Models

- **Linear Q-Network**: A simple linear model for baseline comparison
- **Linear Double Q-Network**: Linear model with Double DQN improvements
- **Deep Q-Network (DQN)**: Standard DQN with convolutional neural network
- **Double DQN**: Reduces overestimation bias by decoupling action selection and evaluation
- **Dueling DQN**: Separates state value and advantage functions for better learning

## ğŸ“ Project Structure

```
â”œâ”€â”€ dqn_atari.py           # Main training script
â”œâ”€â”€ deeprl/
â”‚   â”œâ”€â”€ __init__.py        # Package initialization
â”‚   â”œâ”€â”€ core.py            # Core classes (ReplayMemory, Sample, Preprocessor)
â”‚   â”œâ”€â”€ dqn.py             # DQN Agent implementation
â”‚   â”œâ”€â”€ policy.py          # Policy classes (Îµ-greedy, linear/exponential decay)
â”‚   â”œâ”€â”€ preprocessors.py   # Atari frame preprocessors
â”‚   â”œâ”€â”€ objectives.py      # Loss functions (Huber loss)
â”‚   â””â”€â”€ utils.py           # Utility functions
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ setup.py               # Package setup file
â””â”€â”€ README.md              # This file
```

## ğŸ› ï¸ Installation

### Requirements

- Python >= 3.8
- TensorFlow >= 2.0.0
- OpenAI Gym (with Atari support)
- Gymnasium (with Atari support)
- NumPy
- Pillow
- Matplotlib
- Keras
- SciPy
- h5py
- semver

### Setup

1. Clone the repository:
```bash
git clone https://github.com/seealake/Atari-RL-implementation-in-Spaceinvader.git
cd Atari-RL-implementation-in-Spaceinvader
```

2. Install dependencies:
```bash
pip install -r requirements.txt
pip install -e .
```

## ğŸš€ Usage

### Training

Train a DQN agent with different model architectures:

```bash
# Standard DQN
python dqn_atari.py --mode deep --iterations 1000000

# Double DQN
python dqn_atari.py --mode double --iterations 1000000

# Dueling DQN
python dqn_atari.py --mode dueling --iterations 1000000

# Linear Q-Network (baseline)
python dqn_atari.py --mode linear --iterations 1000000
```

### Command Line Arguments

| Argument | Default | Description |
|----------|---------|-------------|
| `--env` | `ALE/SpaceInvaders-v5` | Atari environment name |
| `--mode` | Required | Model type: `linear`, `linear_double`, `deep`, `double`, `dueling` |
| `--iterations` | 1000000 | Number of training iterations |
| `--output` | `atari-v0` | Output directory for results |
| `--seed` | 0 | Random seed for reproducibility |
| `--checkpoint_dir` | `checkpoints` | Directory for saving checkpoints |
| `--checkpoint_freq` | 50000 | Checkpoint saving frequency |
| `--restart_freq` | 500000 | Training restart frequency |
| `--start_step` | 0 | Step to resume training from |
| `--checkpoint_file` | None | Specific checkpoint file to load |

### Resume Training

```bash
# Resume from a specific step
python dqn_atari.py --mode deep --start_step 100000

# Resume from a specific checkpoint file
python dqn_atari.py --mode deep --checkpoint_file checkpoint_100000.pkl
```

## ğŸ”§ Key Features

- **Experience Replay**: Stores transitions in a replay buffer for stable training
- **Target Network**: Uses a separate target network with soft updates for stability
- **Frame Preprocessing**: Converts frames to grayscale, resizes to 84x84, and stacks 4 frames
- **Reward Clipping**: Clips rewards to [-1, 1] for stable gradients
- **Îµ-Greedy Exploration**: Supports both linear and exponential decay schedules
- **Checkpointing**: Automatic saving and loading of training progress
- **GPU Support**: Automatic GPU detection and memory growth configuration

## ğŸ“Š Training Outputs

During training, the following outputs are generated:

- `Training_loss.png`: Smoothed training loss curve
- `{mode}_learning_curve.png`: Learning curve showing mean reward over time
- `final_results.txt`: Final evaluation results
- `dqn_training.log`: Detailed training logs
- `checkpoints/`: Model checkpoints for resuming training
- `videos/`: Recorded gameplay videos

## ğŸ§  Algorithm Details

### DQN Architecture

The convolutional neural network architecture follows the original DQN paper:
- Conv2D: 32 filters, 8x8 kernel, stride 4, ReLU
- Conv2D: 64 filters, 4x4 kernel, stride 2, ReLU
- Conv2D: 64 filters, 3x3 kernel, stride 1, ReLU
- Dense: 512 units, ReLU
- Dense: num_actions (output layer)

### Hyperparameters

| Parameter | Default Value | Description |
|-----------|---------------|-------------|
| Discount Factor (Î³) | 0.99 | Future reward discount |
| Learning Rate | 0.00025 (RMSprop) | Optimizer learning rate |
| Replay Buffer Size | 500,000 | Maximum experiences stored |
| Batch Size | 64 (in main script) | Training batch size |
| Target Update Frequency | 5,000 steps | Steps between target network updates |
| Burn-in Period | 50,000 steps | Steps before training starts |
| Training Frequency | Every 4 steps | Steps between gradient updates |
| Soft Update Ï„ | 0.001 | Target network soft update weight |
| Initial Îµ | 1.0 | Starting exploration rate |
| Final Îµ | 0.05 | Minimum exploration rate |
| Îµ Decay Rate | 1e-5 | Exponential decay rate for Îµ |

> **Note**: The `DQNAgent` class has a default `batch_size=32`, but `dqn_atari.py` overrides this to `batch_size=64`.

## ğŸ“ License

This project is for educational and research purposes.

## ğŸ”— References

- [Human-Level Control Through Deep Reinforcement Learning](https://www.nature.com/articles/nature14236) - Mnih et al., 2015
- [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461) - van Hasselt et al., 2015
- [Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/abs/1511.06581) - Wang et al., 2015
